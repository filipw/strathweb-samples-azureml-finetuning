{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning and Deploying Phi-4 for PII Extraction\n",
    "\n",
    "This notebook walks through the entire process of fine-tuning the `microsoft/Phi-4-mini-instruct` model on a custom PII dataset, registering it, and deploying it to a managed online endpoint using the Azure ML SDK v2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports\n",
    "\n",
    "First, we'll import the necessary libraries and load the configuration from our `.env` file. This file contains your Azure subscription details and resource names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.ml import MLClient, command, Input, Output\n",
    "from azure.ai.ml.entities import (\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    "    Data,\n",
    "    AmlCompute\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve configuration values\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\")\n",
    "cluster_name = os.getenv(\"CLUSTER_NAME\")\n",
    "vm_size = os.getenv(\"VM_SIZE\")\n",
    "min_nodes = int(os.getenv(\"MIN_NODES\", 0))\n",
    "max_nodes = int(os.getenv(\"MAX_NODES\", 1))\n",
    "endpoint_name = os.getenv(\"ENDPOINT_NAME\")\n",
    "deployment_name = os.getenv(\"DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Connect to Azure ML Workspace\n",
    "\n",
    "Using the loaded credentials, we create an `MLClient` object to interact with our workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace_name\n",
    ")\n",
    "\n",
    "print(f\"Connected to workspace: {ml_client.workspace_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create or Get Compute Cluster\n",
    "\n",
    "We need a GPU-powered compute cluster to run the fine-tuning job. This cell checks if a cluster with the specified name already exists. If not, it creates a new one. This step can take a few minutes if the cluster is being provisioned for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gpu_cluster = ml_client.compute.get(cluster_name)\n",
    "    print(f\"Found existing cluster '{cluster_name}'.\")\n",
    "except ResourceNotFoundError:\n",
    "    print(f\"Cluster '{cluster_name}' not found. Creating a new one...\")\n",
    "    gpu_cluster = AmlCompute(\n",
    "        name=cluster_name,\n",
    "        type=\"amlcompute\",\n",
    "        size=vm_size,\n",
    "        min_instances=min_nodes,\n",
    "        max_instances=max_nodes,\n",
    "        tier=\"LowPriority\",\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(gpu_cluster).result()\n",
    "    print(f\"Cluster '{cluster_name}' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create Data Assets\n",
    "\n",
    "Next, we upload our local `pii_train.jsonl` and `pii_eval.jsonl` files to Azure ML and register them as Data Assets. This makes them accessible and versioned within the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_asset = Data(\n",
    "    path=\"./data/pii_train.jsonl\",\n",
    "    type=\"uri_file\",\n",
    "    name=\"pii_train_data\",\n",
    "    description=\"Training data for PII detection.\",\n",
    ")\n",
    "ml_client.data.create_or_update(train_data_asset)\n",
    "print(f\"Data asset '{train_data_asset.name}' created.\")\n",
    "\n",
    "eval_data_asset = Data(\n",
    "    path=\"./data/pii_eval.jsonl\",\n",
    "    type=\"uri_file\",\n",
    "    name=\"pii_eval_data\",\n",
    "    description=\"Evaluation data for PII detection.\",\n",
    ")\n",
    "ml_client.data.create_or_update(eval_data_asset)\n",
    "print(f\"Data asset '{eval_data_asset.name}' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define the Training Job\n",
    "\n",
    "Here, we define the `command` job. This specifies:\n",
    "- The code to run (`train/train.py`).\n",
    "- The command-line arguments, including inputs and outputs.\n",
    "- The compute target (our GPU cluster).\n",
    "- The software environment needed to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_job_environment = Environment(\n",
    "    image=\"mcr.microsoft.com/azureml/curated/acpt-pytorch-2.2-cuda12.1:latest\",\n",
    "    conda_file=\"./train/environment.yml\",\n",
    ")\n",
    "\n",
    "job_name = f\"phi4-pii-finetune_{int(time.time())}\"\n",
    "\n",
    "job_command = (\n",
    "    \"python train.py \"\n",
    "    \"--model_id microsoft/phi-4 \"\n",
    "    \"--train_data ${{inputs.train_data}} \"\n",
    "    \"--eval_data ${{inputs.eval_data}} \"\n",
    "    \"--model_output ${{outputs.model_output}} \"\n",
    "    \"--save_merged_model True \"\n",
    "    \"--epochs 3 \"\n",
    "    \"--learning_rate 2e-5 \"\n",
    "    \"--gradient_accumulation_steps 2 \"\n",
    ")\n",
    "\n",
    "train_job = command(\n",
    "    name=job_name,\n",
    "    code=\"./train\",\n",
    "    command=job_command,\n",
    "    inputs={\n",
    "        \"train_data\": Input(type=\"uri_file\", path=train_data_asset.path),\n",
    "        \"eval_data\": Input(type=\"uri_file\", path=eval_data_asset.path),\n",
    "    },\n",
    "    outputs={\"model_output\": Output(type=\"uri_folder\")},\n",
    "    environment=custom_job_environment,\n",
    "    compute=cluster_name,\n",
    "    display_name=\"Fine-tune Phi-4 (microsoft/phi-4) for PII Derection\",\n",
    "    experiment_name=\"phi4-pii-finetuning\",\n",
    ")\n",
    "\n",
    "print(f\"Training job '{job_name}' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Submit and Stream the Training Job\n",
    "\n",
    "This is the main training step. We submit the job defined above to Azure ML. The `.stream()` method will display the logs from the remote compute cluster directly in the notebook's output. \n",
    "\n",
    "**This will take a significant amount of time (e.g., 30-60+ minutes).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Submitting training job: {job_name}\")\n",
    "returned_job = ml_client.jobs.create_or_update(train_job)\n",
    "ml_client.jobs.stream(returned_job.name)\n",
    "print(f\"Training job '{returned_job.name}' completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Register the Model\n",
    "\n",
    "Once the training job is complete, the fine-tuned model artifacts are stored in the job's output. We now register these artifacts as a versioned Model in the Azure ML workspace, which makes it easy to track and deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"phi-4-large-pii-model\"\n",
    "model_path = f\"azureml://jobs/{returned_job.name}/outputs/model_output\"\n",
    "\n",
    "registered_model = ml_client.models.create_or_update(\n",
    "    Model(path=model_path, name=model_name, description=\"Fine-tuned Phi-4 (microsoft/phi-4) for PII detection.\")\n",
    ")\n",
    "print(f\"Model '{registered_model.name}' version '{registered_model.version}' registered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Create Online Endpoint\n",
    "\n",
    "An endpoint is an HTTPS endpoint that clients can call to get predictions from your model. We create a 'batch' endpoint, because such ones can be created with low priority (spot VMs), for others you need dedicated VMs which are not only more expensive, but typically require organizational accounts. Azure handles the underlying infrastructure.\n",
    "\n",
    "We also deploy a model to the endpoint. A deployment is a set of resources required for hosting the model. This step provisions the compute, deploys the model, and configures the scoring logic.\n",
    "\n",
    "**This step will also take several minutes to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    BatchEndpoint,\n",
    "    BatchDeployment,\n",
    "    BatchRetrySettings,\n",
    ")\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "import time\n",
    "\n",
    "# Create a unique name for the batch endpoint\n",
    "batch_endpoint_name = f\"pii-batch-{int(time.time())}\"\n",
    "\n",
    "# Create the endpoint\n",
    "print(f\"Creating batch endpoint '{batch_endpoint_name}'...\")\n",
    "endpoint = BatchEndpoint(\n",
    "    name=batch_endpoint_name,\n",
    "    description=\"Batch endpoint for PII extraction with the fine-tuned Phi-4 model.\",\n",
    ")\n",
    "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()\n",
    "print(\"Batch endpoint created successfully.\")\n",
    "\n",
    "retry_settings = BatchRetrySettings(max_retries=1, timeout=3600)\n",
    "\n",
    "# Create the deployment\n",
    "print(\"Creating batch deployment...\")\n",
    "deployment = BatchDeployment(\n",
    "    name=\"pii-batch-deployment\",\n",
    "    endpoint_name=batch_endpoint_name,\n",
    "    model=registered_model,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"./deployment\",\n",
    "        scoring_script=\"score.py\",\n",
    "    ),\n",
    "    environment=Environment(\n",
    "        conda_file=\"./deployment/environment.yml\",\n",
    "        image=\"mcr.microsoft.com/azureml/curated/acpt-pytorch-2.2-cuda12.1:latest\"\n",
    "    ),\n",
    "    compute=cluster_name,\n",
    "    instance_count=1,\n",
    "    max_concurrency_per_instance=1,\n",
    "    mini_batch_size=1,\n",
    "    logging_level=\"INFO\",\n",
    "    retry_settings=retry_settings,\n",
    ")\n",
    "ml_client.batch_deployments.begin_create_or_update(deployment).result()\n",
    "print(\"Batch deployment created successfully.\")\n",
    "\n",
    "# Set the default deployment for the endpoint\n",
    "endpoint = ml_client.batch_endpoints.get(batch_endpoint_name)\n",
    "endpoint.defaults.deployment_name = deployment.name\n",
    "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()\n",
    "print(\"Default deployment set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Invoke the Batch Endpoint\n",
    "\n",
    "Finally, let's send a send our evaluation dataset to the endpoint for batch scoring. The results will be saved to a specified output in Azure ML, and will also be downloaded and displayed in the notebook. \n",
    "\n",
    "Note that this is a batch endpoint, based on Low Priority VMs, so it may take some time for the underlying job to start and complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Step 1: Define the Input for the Job ---\n",
    "# The input for the job is our evaluation data asset\n",
    "input_data = Input(type=\"uri_file\", path=eval_data_asset.path)\n",
    "\n",
    "# --- Step 2: Invoke the Endpoint to Kick Off the Batch Job ---\n",
    "print(f\"Invoking batch endpoint '{batch_endpoint_name}'...\")\n",
    "job = ml_client.batch_endpoints.invoke(\n",
    "    endpoint_name=batch_endpoint_name,\n",
    "    input=input_data,\n",
    ")\n",
    "print(f\"Batch job '{job.name}' started. Waiting for completion...\")\n",
    "\n",
    "# --- Step 3: Stream Logs and Wait for the Job to Finish ---\n",
    "ml_client.jobs.stream(job.name)\n",
    "print(\"Batch job finished.\")\n",
    "\n",
    "# --- Step 4: Download the Results ---\n",
    "output_dir = \"./batch_results\"\n",
    "print(\"Downloading results...\")\n",
    "ml_client.jobs.download(name=job.name, download_path=output_dir)\n",
    "print(f\"Results downloaded to {output_dir}\")\n",
    "\n",
    "# --- Step 5: Find and Display the Raw Output File ---\n",
    "output_files = glob.glob(f\"{output_dir}/**/predictions.jsonl\", recursive=True)\n",
    "\n",
    "if output_files:\n",
    "    print(f\"Found output file at: {output_files[0]}\")\n",
    "    \n",
    "    print(\"\\n--- Raw Batch Scoring Results (First 5 lines) ---\")\n",
    "    with open(output_files[0], 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 5:\n",
    "                break\n",
    "            print(f\"--- Record {i+1} ---\\n{line.strip()}\\n\")\n",
    "else:\n",
    "    print(\"Could not find the output JSONL file in the results folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Cleanup\n",
    "\n",
    "Run these commands in your terminal to delete the endpoint and compute cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"To clean up, run the following commands in your terminal:\")\n",
    "print(f\"\\naz ml online-endpoint delete --name {endpoint_name} --yes\")\n",
    "print(f\"az ml compute delete --name {cluster_name} --yes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
